{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f869122-c9e1-40fe-bfc2-6acb60867c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¾èµ–åº“å¯¼å…¥å®Œæˆï¼\n",
      "MindSporeç‰ˆæœ¬ï¼š1.7.0\n"
     ]
    }
   ],
   "source": [
    "# =========== ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å…¥æ‰€æœ‰ä¾èµ–åº“ ===========\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "\n",
    "# è®¾ç½®MindSporeè¿è¡Œç¯å¢ƒ\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"GPU\")\n",
    "print(\"âœ… ä¾èµ–åº“å¯¼å…¥å®Œæˆï¼\")\n",
    "print(f\"MindSporeç‰ˆæœ¬ï¼š{ms.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56da1328-d1e0-4bec-9892-811b33315d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-10 15:00:53--  https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com/deep-learning/auto-mpg.data\n",
      "Resolving proxy.modelarts.com (proxy.modelarts.com)... 192.168.6.3\n",
      "Connecting to proxy.modelarts.com (proxy.modelarts.com)|192.168.6.3|:80... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 30286 (30K) [binary/octet-stream]\n",
      "Saving to: â€˜auto-mpg.data.1â€™\n",
      "\n",
      "auto-mpg.data.1     100%[===================>]  29.58K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-12-10 15:00:53 (21.9 MB/s) - â€˜auto-mpg.data.1â€™ saved [30286/30286]\n",
      "\n",
      "âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# =========== ç¬¬äºŒéƒ¨åˆ†ï¼šä¸‹è½½æ•°æ®é›† ===========\n",
    "# è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡\n",
    "import os\n",
    "os.environ['no_proxy'] = 'a.test.com,127.0.0.1,2.2.2.2'\n",
    "\n",
    "# ä¸‹è½½æ•°æ®é›†\n",
    "!wget https://ascend-professional-construction-dataset.obs.cn-north-4.myhuaweicloud.com/deep-learning/auto-mpg.data\n",
    "\n",
    "print(\"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "610be5e8-ad55-4aa3-836e-23cbf217aa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æŸ¥çœ‹åŸå§‹æ•°æ®æ ¼å¼\n",
      "==================================================\n",
      "æ•°æ®é›†æ€»è¡Œæ•°ï¼š398\n",
      "\n",
      "ç¤ºä¾‹æ•°æ®ï¼ˆç¬¬20-25è¡Œï¼‰ï¼š\n",
      "è¡Œ21: ['25.0   4   110.0      87.00      2672.      17.5   70  2\\t\"peugeot 504\"']\n",
      "è¡Œ22: ['24.0   4   107.0      90.00      2430.      14.5   70  2\\t\"audi 100 ls\"']\n",
      "è¡Œ23: ['25.0   4   104.0      95.00      2375.      17.5   70  2\\t\"saab 99e\"']\n",
      "è¡Œ24: ['26.0   4   121.0      113.0      2234.      12.5   70  2\\t\"bmw 2002\"']\n",
      "è¡Œ25: ['21.0   6   199.0      90.00      2648.      15.0   70  1\\t\"amc gremlin\"']\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæŸ¥çœ‹åŸå§‹æ•°æ®æ ¼å¼ ====================\n",
    "print(\"ğŸ“Š æŸ¥çœ‹åŸå§‹æ•°æ®æ ¼å¼\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with open('auto-mpg.data') as csv_file:\n",
    "    data_lines = list(csv.reader(csv_file, delimiter=','))\n",
    "    \n",
    "print(f\"æ•°æ®é›†æ€»è¡Œæ•°ï¼š{len(data_lines)}\")\n",
    "print(\"\\nç¤ºä¾‹æ•°æ®ï¼ˆç¬¬20-25è¡Œï¼‰ï¼š\")\n",
    "for i in range(20, 25):\n",
    "    print(f\"è¡Œ{i+1}: {data_lines[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b913510c-ef91-4318-a2e2-38716bfadc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ ä½¿ç”¨pandasè¯»å–æ•°æ®\n",
      "==================================================\n",
      "æ•°æ®å½¢çŠ¶ï¼š(398, 8)\n",
      "\n",
      "æ•°æ®å‰5è¡Œï¼š\n",
      "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
      "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
      "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
      "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
      "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
      "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
      "\n",
      "   Model Year  Origin  \n",
      "0          70       1  \n",
      "1          70       1  \n",
      "2          70       1  \n",
      "3          70       1  \n",
      "4          70       1  \n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä½¿ç”¨pandasè¯»å–æ•°æ® ====================\n",
    "print(\"ğŸ“‹ ä½¿ç”¨pandasè¯»å–æ•°æ®\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_data = pd.read_csv('auto-mpg.data', \n",
    "                       names=column_names,\n",
    "                       na_values=\"?\", \n",
    "                       comment='\\t',\n",
    "                       sep=\" \", \n",
    "                       skipinitialspace=True)\n",
    "\n",
    "data = raw_data.copy()\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶ï¼š{data.shape}\")\n",
    "print(\"\\næ•°æ®å‰5è¡Œï¼š\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de52500-72a6-4cbd-a316-bc5aa0b399fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å¤„ç†\n",
      "==================================================\n",
      "åŸå§‹æ•°æ®è¡Œæ•°ï¼š398\n",
      "æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼š392\n",
      "\n",
      "æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼š\n",
      "              count         mean         std     min       25%     50%  \\\n",
      "Cylinders     392.0     5.471939    1.705783     3.0     4.000     4.0   \n",
      "Displacement  392.0   194.411990  104.644004    68.0   105.000   151.0   \n",
      "Horsepower    392.0   104.469388   38.491160    46.0    75.000    93.5   \n",
      "Weight        392.0  2977.584184  849.402560  1613.0  2225.250  2803.5   \n",
      "Acceleration  392.0    15.541327    2.758864     8.0    13.775    15.5   \n",
      "Model Year    392.0    75.979592    3.683737    70.0    73.000    76.0   \n",
      "\n",
      "                   75%     max  \n",
      "Cylinders        8.000     8.0  \n",
      "Displacement   275.750   455.0  \n",
      "Horsepower     126.000   230.0  \n",
      "Weight        3614.750  5140.0  \n",
      "Acceleration    17.025    24.8  \n",
      "Model Year      79.000    82.0  \n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®æ¸…æ´—ä¸ç‰¹å¾å¤„ç† ====================\n",
    "print(\"ğŸ§¹ æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å¤„ç†\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# åˆ é™¤ç¼ºå¤±å€¼\n",
    "print(f\"åŸå§‹æ•°æ®è¡Œæ•°ï¼š{len(data)}\")\n",
    "data = data.dropna()\n",
    "print(f\"æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼š{len(data)}\")\n",
    "\n",
    "# åˆ†ç¦»æ ‡ç­¾å’Œç‰¹å¾\n",
    "origin = data.pop('Origin')\n",
    "data_labels = data.pop('MPG')\n",
    "\n",
    "# æ•°æ®ç»Ÿè®¡ä¿¡æ¯\n",
    "train_stats = data.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "print(\"\\næ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼š\")\n",
    "print(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "960f9939-5dda-4af8-8bc3-473d006cc3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ•°æ®å½’ä¸€åŒ–å¤„ç†\n",
      "==================================================\n",
      "å¤„ç†åæ•°æ®å½¢çŠ¶ï¼š(392, 10)\n",
      "ç‰¹å¾åˆ—ï¼š['Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'MPG', 'USA', 'Europe', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šæ•°æ®å½’ä¸€åŒ– ====================\n",
    "print(\"ğŸ“ æ•°æ®å½’ä¸€åŒ–å¤„ç†\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# å®šä¹‰å½’ä¸€åŒ–å‡½æ•°\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_data = norm(data)\n",
    "normed_data['MPG'] = data_labels\n",
    "\n",
    "# One-hotç¼–ç \n",
    "normed_data['USA'] = (origin == 1) * 1.0\n",
    "normed_data['Europe'] = (origin == 2) * 1.0\n",
    "normed_data['Japan'] = (origin == 3) * 1.0\n",
    "\n",
    "print(f\"å¤„ç†åæ•°æ®å½¢çŠ¶ï¼š{normed_data.shape}\")\n",
    "print(f\"ç‰¹å¾åˆ—ï¼š{list(normed_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4e7d43f-e15b-49f6-8819-875f2f9956b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
      "==================================================\n",
      "è®­ç»ƒé›†å¤§å°ï¼š314 è¡Œ\n",
      "æµ‹è¯•é›†å¤§å°ï¼š78 è¡Œ\n",
      "\n",
      "X_trainå½¢çŠ¶ï¼š(314, 9)\n",
      "Y_trainå½¢çŠ¶ï¼š(314,)\n",
      "X_testå½¢çŠ¶ï¼š(78, 9)\n",
      "Y_testå½¢çŠ¶ï¼š(78,)\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† ====================\n",
    "print(\" åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# æŒ‰80%/20%æ¯”ä¾‹åˆ’åˆ†\n",
    "train_dataset = normed_data.sample(frac=0.8, random_state=0)\n",
    "test_dataset = normed_data.drop(train_dataset.index)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°ï¼š{len(train_dataset)} è¡Œ\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°ï¼š{len(test_dataset)} è¡Œ\")\n",
    "\n",
    "# åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾\n",
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')\n",
    "\n",
    "X_train = np.array(train_dataset, dtype=np.float32)\n",
    "Y_train = np.array(train_labels, dtype=np.float32)\n",
    "X_test = np.array(test_dataset, dtype=np.float32)\n",
    "Y_test = np.array(test_labels, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nX_trainå½¢çŠ¶ï¼š{X_train.shape}\")\n",
    "print(f\"Y_trainå½¢çŠ¶ï¼š{Y_train.shape}\")\n",
    "print(f\"X_testå½¢çŠ¶ï¼š{X_test.shape}\")\n",
    "print(f\"Y_testå½¢çŠ¶ï¼š{Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45b8331d-73c2-43ee-afcf-f72b81bedf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " åˆ›å»ºMindSporeæ•°æ®é›†\n",
      "==================================================\n",
      "è®­ç»ƒæ•°æ®é›†å¤§å°ï¼š314\n",
      "æµ‹è¯•æ•°æ®é›†å¤§å°ï¼š78\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬å…«éƒ¨åˆ†ï¼šåˆ›å»ºMindSporeæ•°æ®é›† ====================\n",
    "print(\" åˆ›å»ºMindSporeæ•°æ®é›†\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class Iterable:\n",
    "    def __init__(self, X, Y):\n",
    "        self._data = X\n",
    "        self._label = Y[:, np.newaxis]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._data[index], self._label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "train_iter = Iterable(X_train, Y_train)\n",
    "test_iter = Iterable(X_test, Y_test)\n",
    "\n",
    "dataset_train = GeneratorDataset(source=train_iter, column_names=[\"data\", \"label\"])\n",
    "dataset_test = GeneratorDataset(source=test_iter, column_names=[\"data\", \"label\"])\n",
    "\n",
    "print(f\"è®­ç»ƒæ•°æ®é›†å¤§å°ï¼š{dataset_train.get_dataset_size()}\")\n",
    "print(f\"æµ‹è¯•æ•°æ®é›†å¤§å°ï¼š{dataset_test.get_dataset_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "793603ce-83a1-4587-9169-debec8c4940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  æ„å»ºå…¨è¿æ¥ç¥ç»ç½‘ç»œ\n",
      "==================================================\n",
      "ç¥ç»ç½‘ç»œç»“æ„ï¼š\n",
      "RegressionCar<\n",
      "  (flatten): Flatten<>\n",
      "  (fc1): Dense<\n",
      "    input_channels=9, output_channels=64, has_bias=True, activation=ReLU<>\n",
      "    (activation): ReLU<>\n",
      "    >\n",
      "  (fc2): Dense<\n",
      "    input_channels=64, output_channels=64, has_bias=True, activation=ReLU<>\n",
      "    (activation): ReLU<>\n",
      "    >\n",
      "  (fc3): Dense<input_channels=64, output_channels=1, has_bias=True>\n",
      "  >\n",
      "\n",
      "âœ… ç½‘ç»œæ„å»ºå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬ä¹éƒ¨åˆ†ï¼šæ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ ====================\n",
    "print(\"ğŸ§  æ„å»ºå…¨è¿æ¥ç¥ç»ç½‘ç»œ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class RegressionCar(nn.Cell):\n",
    "    \"\"\"æ±½è½¦æ²¹è€—é¢„æµ‹ç¥ç»ç½‘ç»œ - 3å±‚å…¨è¿æ¥ç½‘ç»œ\"\"\"\n",
    "    def __init__(self):\n",
    "        super(RegressionCar, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(9, 64, activation='relu')   # è¾“å…¥å±‚ï¼š9ä¸ªç‰¹å¾\n",
    "        self.fc2 = nn.Dense(64, 64, activation='relu')  # éšè—å±‚ï¼š64ä¸ªç¥ç»å…ƒ\n",
    "        self.fc3 = nn.Dense(64, 1)                      # è¾“å‡ºå±‚ï¼š1ä¸ªå€¼ï¼ˆMPGé¢„æµ‹ï¼‰\n",
    "    \n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# å®ä¾‹åŒ–ç½‘ç»œ\n",
    "network = RegressionCar()\n",
    "\n",
    "print(\"ç¥ç»ç½‘ç»œç»“æ„ï¼š\")\n",
    "print(network)\n",
    "print(\"\\nâœ… ç½‘ç»œæ„å»ºå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da9827f4-ea77-46af-a0aa-8ab1e96c6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " é…ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
      "==================================================\n",
      "æŸå¤±å‡½æ•°ï¼šMSELoss<>\n",
      "ä¼˜åŒ–å™¨ï¼šRMSPropï¼Œå­¦ä¹ ç‡ï¼š0.001\n",
      "å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼š6\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬åéƒ¨åˆ†ï¼šå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ====================\n",
    "print(\" é…ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ä½¿ç”¨æ–‡æ¡£ä¸­çš„é…ç½®\n",
    "net_loss = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±\n",
    "net_opt = nn.RMSProp(network.trainable_params(), learning_rate=0.001)  # RMSPropä¼˜åŒ–å™¨\n",
    "\n",
    "print(f\"æŸå¤±å‡½æ•°ï¼š{net_loss}\")\n",
    "print(f\"ä¼˜åŒ–å™¨ï¼šRMSPropï¼Œå­¦ä¹ ç‡ï¼š0.001\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼š{len(network.trainable_params())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed1309a2-a7d8-45b0-98fd-2ea6fa21ed87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‹ï¸ å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°\n",
      "==================================================\n",
      "âœ… è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬åä¸€éƒ¨åˆ†ï¼šå®šä¹‰è®­ç»ƒå‡½æ•° ====================\n",
    "print(\"ğŸ‹ï¸ å®šä¹‰æ¨¡å‹è®­ç»ƒå‡½æ•°\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def train_loop(model, dataset, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    è®­ç»ƒå¾ªç¯å‡½æ•° - ä½¿ç”¨TrainOneStepCellï¼ˆMindSpore 1.7å…¼å®¹ï¼‰\n",
    "    å‚æ•°ï¼š\n",
    "        model: ç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "        dataset: è®­ç»ƒæ•°æ®é›†\n",
    "        loss_fn: æŸå¤±å‡½æ•°\n",
    "        optimizer: ä¼˜åŒ–å™¨\n",
    "    è¿”å›ï¼š\n",
    "        å¹³å‡æŸå¤±\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºè®­ç»ƒç½‘ç»œ\n",
    "    class TrainNet(nn.Cell):\n",
    "        def __init__(self, network, loss_fn):\n",
    "            super(TrainNet, self).__init__()\n",
    "            self.network = network\n",
    "            self.loss_fn = loss_fn\n",
    "            \n",
    "        def construct(self, data, label):\n",
    "            output = self.network(data)\n",
    "            return self.loss_fn(output, label)\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒç½‘ç»œå’Œä¸€æ­¥è®­ç»ƒå™¨\n",
    "    train_net = TrainNet(model, loss_fn)\n",
    "    train_step = nn.TrainOneStepCell(train_net, optimizer)\n",
    "    \n",
    "    # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "    train_step.set_train(True)\n",
    "    \n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    size = dataset.get_dataset_size()\n",
    "    \n",
    "    print(\"  å¼€å§‹è®­ç»ƒå¾ªç¯...\")\n",
    "    \n",
    "    # éå†æ•°æ®é›†\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        # è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆä¸æ–‡æ¡£ä¸€è‡´ï¼‰\n",
    "        data_tensor = Tensor(data[:, np.newaxis].T, ms.float32)\n",
    "        label_tensor = Tensor(label[:, np.newaxis], ms.float32)\n",
    "        \n",
    "        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤\n",
    "        loss = train_step(data_tensor, label_tensor)\n",
    "        \n",
    "        # ç´¯åŠ æŸå¤±\n",
    "        total_loss += loss.asnumpy()\n",
    "        batch_count += 1\n",
    "        \n",
    "        # æ¯100ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡æŸå¤±ï¼ˆä¸æ–‡æ¡£ä¸€è‡´ï¼‰\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡æŸå¤±\n",
    "    avg_loss = total_loss / batch_count\n",
    "    return avg_loss\n",
    "\n",
    "print(\"âœ… è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b4dd6aa-c733-4fe4-8416-3712de8e20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª å®šä¹‰æ¨¡å‹æµ‹è¯•å‡½æ•°\n",
      "==================================================\n",
      "âœ… æµ‹è¯•å‡½æ•°å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬åäºŒéƒ¨åˆ†ï¼šå®šä¹‰æµ‹è¯•å‡½æ•° ====================\n",
    "print(\"ğŸ§ª å®šä¹‰æ¨¡å‹æµ‹è¯•å‡½æ•°\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    \"\"\"\n",
    "    æµ‹è¯•å¾ªç¯å‡½æ•° - ä¿®æ­£å›å½’ä»»åŠ¡çš„è¯„ä¼°\n",
    "    å‚æ•°ï¼š\n",
    "        model: ç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "        dataset: æµ‹è¯•æ•°æ®é›†\n",
    "        loss_fn: æŸå¤±å‡½æ•°\n",
    "    è¿”å›ï¼š\n",
    "        å¹³å‡æŸå¤±\n",
    "    \"\"\"\n",
    "    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    model.set_train(False)\n",
    "    \n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        # è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰\n",
    "        data_tensor = Tensor(data[:, np.newaxis].T, ms.float32)\n",
    "        label_tensor = Tensor(label[:, np.newaxis], ms.float32)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­ï¼ˆé¢„æµ‹ï¼‰\n",
    "        pred = model(data_tensor)\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±\n",
    "        loss = loss_fn(pred, label_tensor)\n",
    "        total_loss += loss.asnumpy()\n",
    "        count += 1\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡æŸå¤±\n",
    "    avg_loss = total_loss / count\n",
    "    return avg_loss\n",
    "\n",
    "print(\"âœ… æµ‹è¯•å‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6e44de3-b135-4fa6-8787-2c045440c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
      "==================================================\n",
      "è®­ç»ƒé…ç½®ï¼š\n",
      "  è®­ç»ƒè½®æ•°ï¼š10\n",
      "  è®­ç»ƒæ ·æœ¬æ•°ï¼š314\n",
      "  æµ‹è¯•æ ·æœ¬æ•°ï¼š78\n",
      "  å¼€å§‹æ—¶é—´ï¼š2025-12-10 15:11:03\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 1\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 3.988396  [  0/314]\n",
      "loss: 4.194922  [100/314]\n",
      "loss: 10.996804  [200/314]\n",
      "loss: 92.740524  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 8.194653\n",
      " æµ‹è¯•æŸå¤±: 7.126061\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 1.291683  [  0/314]\n",
      "loss: 0.906311  [100/314]\n",
      "loss: 13.269925  [200/314]\n",
      "loss: 9.124539  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 7.985721\n",
      " æµ‹è¯•æŸå¤±: 9.685973\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 2.334242  [  0/314]\n",
      "loss: 1.757432  [100/314]\n",
      "loss: 13.804032  [200/314]\n",
      "loss: 1.178879  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 8.033397\n",
      " æµ‹è¯•æŸå¤±: 7.348125\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 2.825895  [  0/314]\n",
      "loss: 0.201715  [100/314]\n",
      "loss: 2.630815  [200/314]\n",
      "loss: 3.615944  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 8.107369\n",
      " æµ‹è¯•æŸå¤±: 6.838921\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 0.904804  [  0/314]\n",
      "loss: 1.322302  [100/314]\n",
      "loss: 0.622319  [200/314]\n",
      "loss: 3.986812  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 7.555185\n",
      " æµ‹è¯•æŸå¤±: 8.493803\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 39.909615  [  0/314]\n",
      "loss: 0.300702  [100/314]\n",
      "loss: 3.444831  [200/314]\n",
      "loss: 0.134350  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 7.624567\n",
      " æµ‹è¯•æŸå¤±: 8.070809\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 2.040877  [  0/314]\n",
      "loss: 3.915158  [100/314]\n",
      "loss: 0.517188  [200/314]\n",
      "loss: 0.032754  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 8.041963\n",
      " æµ‹è¯•æŸå¤±: 6.813209\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 0.008147  [  0/314]\n",
      "loss: 52.781769  [100/314]\n",
      "loss: 0.150870  [200/314]\n",
      "loss: 2.854606  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 7.479737\n",
      " æµ‹è¯•æŸå¤±: 13.542217\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 6.164653  [  0/314]\n",
      "loss: 13.481378  [100/314]\n",
      "loss: 0.840889  [200/314]\n",
      "loss: 1.232686  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 7.734927\n",
      " æµ‹è¯•æŸå¤±: 6.126594\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "  å¼€å§‹è®­ç»ƒå¾ªç¯...\n",
      "loss: 0.776095  [  0/314]\n",
      "loss: 1.158643  [100/314]\n",
      "loss: 0.007908  [200/314]\n",
      "loss: 3.307215  [300/314]\n",
      " è®­ç»ƒæŸå¤±: 7.491502\n",
      " æµ‹è¯•æŸå¤±: 9.928147\n",
      "\n",
      "==================================================\n",
      "ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼\n",
      "ç»“æŸæ—¶é—´ï¼š2025-12-10 15:11:16\n",
      "æœ€ç»ˆè®­ç»ƒæŸå¤±ï¼š7.4915\n",
      "æœ€ç»ˆæµ‹è¯•æŸå¤±ï¼š9.9281\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬åä¸‰éƒ¨:è®­ç»ƒæ¨¡å‹ ====================\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒè½®æ•°\n",
    "epochs = 10\n",
    "\n",
    "print(f\"è®­ç»ƒé…ç½®ï¼š\")\n",
    "print(f\"  è®­ç»ƒè½®æ•°ï¼š{epochs}\")\n",
    "print(f\"  è®­ç»ƒæ ·æœ¬æ•°ï¼š{dataset_train.get_dataset_size()}\")\n",
    "print(f\"  æµ‹è¯•æ ·æœ¬æ•°ï¼š{dataset_test.get_dataset_size()}\")\n",
    "print(f\"  å¼€å§‹æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# è®­ç»ƒå†å²è®°å½•\n",
    "train_loss_history = []  # è®°å½•æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±\n",
    "test_loss_history = []   # è®°å½•æ¯ä¸ªepochçš„æµ‹è¯•æŸå¤±\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # è®­ç»ƒé˜¶æ®µ\n",
    "    train_loss = train_loop(network, dataset_train, net_loss, net_opt)\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    # æµ‹è¯•é˜¶æ®µ\n",
    "    test_loss = test_loop(network, dataset_test, net_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "    \n",
    "    # æ‰“å°å½“å‰epochç»“æœ\n",
    "    print(f\" è®­ç»ƒæŸå¤±: {train_loss:>8f}\")\n",
    "    print(f\" æµ‹è¯•æŸå¤±: {test_loss:>8f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"ç»“æŸæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"æœ€ç»ˆè®­ç»ƒæŸå¤±ï¼š{train_loss_history[-1]:.4f}\")\n",
    "print(f\"æœ€ç»ˆæµ‹è¯•æŸå¤±ï¼š{test_loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39676f7a-9ed1-47c9-a48b-b570d431cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ¨¡å‹é¢„æµ‹ä¸è¯„ä¼°\n",
      "==================================================\n",
      "1. é¢„æµ‹ç¤ºä¾‹ï¼š\n",
      "------------------------------\n",
      "æ ·æœ¬ 1:\n",
      "  çœŸå®MPGå€¼: 15.00\n",
      "  é¢„æµ‹MPGå€¼: 13.99\n",
      "  è¯¯å·®: 1.01\n",
      "\n",
      "æ ·æœ¬ 2:\n",
      "  çœŸå®MPGå€¼: 10.00\n",
      "  é¢„æµ‹MPGå€¼: 13.04\n",
      "  è¯¯å·®: 3.04\n",
      "\n",
      "æ ·æœ¬ 3:\n",
      "  çœŸå®MPGå€¼: 9.00\n",
      "  é¢„æµ‹MPGå€¼: 11.56\n",
      "  è¯¯å·®: 2.56\n",
      "\n",
      "æ ·æœ¬ 4:\n",
      "  çœŸå®MPGå€¼: 25.00\n",
      "  é¢„æµ‹MPGå€¼: 26.13\n",
      "  è¯¯å·®: 1.13\n",
      "\n",
      "æ ·æœ¬ 5:\n",
      "  çœŸå®MPGå€¼: 19.00\n",
      "  é¢„æµ‹MPGå€¼: 20.65\n",
      "  è¯¯å·®: 1.65\n",
      "\n",
      "\n",
      "2. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼š\n",
      "------------------------------\n",
      "å¹³å‡ç»å¯¹è¯¯å·®(MAE): 2.4346\n",
      "å‡æ–¹è¯¯å·®(MSE): 9.9281\n",
      "å‡æ–¹æ ¹è¯¯å·®(RMSE): 3.1509\n",
      "\n",
      "âœ… å®éªŒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ==================== ç¬¬åå››éƒ¨ï¼šé¢„æµ‹ä¸è¯„ä¼° ====================\n",
    "print(\"ğŸ” æ¨¡å‹é¢„æµ‹ä¸è¯„ä¼°\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"1. é¢„æµ‹ç¤ºä¾‹ï¼š\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "network.set_train(False)\n",
    "\n",
    "# å¯¹å‰5ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹\n",
    "for i in range(5):\n",
    "    sample_data = Tensor(X_test[i:i+1], ms.float32).reshape(1, 1, -1)\n",
    "    prediction = network(sample_data)\n",
    "    pred_value = prediction.asnumpy()[0][0]\n",
    "    \n",
    "    print(f\"æ ·æœ¬ {i+1}:\")\n",
    "    print(f\"  çœŸå®MPGå€¼: {Y_test[i]:.2f}\")\n",
    "    print(f\"  é¢„æµ‹MPGå€¼: {pred_value:.2f}\")\n",
    "    print(f\"  è¯¯å·®: {abs(Y_test[i] - pred_value):.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n2. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼š\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# å¯¹æ•´ä¸ªæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹\n",
    "all_predictions = []\n",
    "for i in range(len(X_test)):\n",
    "    sample_data = Tensor(X_test[i:i+1], ms.float32).reshape(1, 1, -1)\n",
    "    pred = network(sample_data)\n",
    "    all_predictions.append(pred.asnumpy()[0][0])\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# è®¡ç®—MAEå’ŒMSEï¼ˆä½¿ç”¨numpyè®¡ç®—ï¼‰\n",
    "mae = np.mean(np.abs(Y_test - all_predictions))\n",
    "mse = np.mean((Y_test - all_predictions) ** 2)\n",
    "\n",
    "print(f\"å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.4f}\")\n",
    "print(f\"å‡æ–¹è¯¯å·®(MSE): {mse:.4f}\")\n",
    "print(f\"å‡æ–¹æ ¹è¯¯å·®(RMSE): {np.sqrt(mse):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… å®éªŒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7405b0-c8bd-44c7-aeea-cc8a14a302c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ç¬¬åäº”éƒ¨åˆ†ï¼šå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ ====================\n",
    "print(\"ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# åˆ›å»ºå›¾å½¢\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "axes[0, 0].plot(train_loss_history, label='è®­ç»ƒæŸå¤±', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(test_loss_history, label='æµ‹è¯•æŸå¤±', color='red', linewidth=2)\n",
    "axes[0, 0].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('æŸå¤±å€¼ (MSE)', fontsize=12)\n",
    "axes[0, 0].set_title('è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xticks(range(epochs))\n",
    "axes[0, 0].set_xticklabels([f'E{i+1}' for i in range(epochs)])\n",
    "\n",
    "# 2. æŸå¤±ä¸‹é™é€Ÿåº¦ï¼ˆå¯¹æ•°åæ ‡ï¼‰\n",
    "axes[0, 1].semilogy(train_loss_history, label='è®­ç»ƒæŸå¤±', color='blue', linewidth=2, marker='o')\n",
    "axes[0, 1].semilogy(test_loss_history, label='æµ‹è¯•æŸå¤±', color='red', linewidth=2, marker='s')\n",
    "axes[0, 1].set_xlabel('è®­ç»ƒè½®æ•° (Epoch)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('æŸå¤±å€¼ (MSE, å¯¹æ•°åæ ‡)', fontsize=12)\n",
    "axes[0, 1].set_title('æŸå¤±ä¸‹é™è¶‹åŠ¿ï¼ˆå¯¹æ•°åæ ‡ï¼‰', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# è®¡ç®—æ‰€æœ‰æµ‹è¯•æ ·æœ¬çš„é¢„æµ‹å€¼\n",
    "print(\"æ­£åœ¨è®¡ç®—é¢„æµ‹å€¼ç”¨äºå¯è§†åŒ–...\")\n",
    "all_predictions = []\n",
    "network.set_train(False)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    sample_data = Tensor(X_test[i:i+1], ms.float32).reshape(1, 1, -1)\n",
    "    pred = network(sample_data)\n",
    "    all_predictions.append(pred.asnumpy()[0][0])\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# 3. é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾\n",
    "axes[1, 0].scatter(Y_test, all_predictions, alpha=0.6, color='green', s=50)\n",
    "# ç»˜åˆ¶ç†æƒ³é¢„æµ‹çº¿ï¼ˆy=xï¼‰\n",
    "min_val = min(Y_test.min(), all_predictions.min())\n",
    "max_val = max(Y_test.max(), all_predictions.max())\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='ç†æƒ³é¢„æµ‹')\n",
    "axes[1, 0].set_xlabel('çœŸå®MPGå€¼', fontsize=12)\n",
    "axes[1, 0].set_ylabel('é¢„æµ‹MPGå€¼', fontsize=12)\n",
    "axes[1, 0].set_title('é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾\n",
    "errors = Y_test - all_predictions\n",
    "axes[1, 1].hist(errors, bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='é›¶è¯¯å·®çº¿')\n",
    "axes[1, 1].set_xlabel('é¢„æµ‹è¯¯å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('é¢‘æ•°', fontsize=12)\n",
    "axes[1, 1].set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# è°ƒæ•´å¸ƒå±€å¹¶æ˜¾ç¤º\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… å¯è§†åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d820e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
